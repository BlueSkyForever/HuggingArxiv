# [iScore 是一款可视化的分析工具，专门用于揭示语言模型如何自动评估和打分摘要内容，助力理解其内在评分机制。](https://arxiv.org/abs/2403.04760)

发布时间：2024年03月07日

`LLM应用`

> iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries

> 随着LLMs的广泛应用热潮，学习工程师们受到启发，正尝试将它们融入能自动评分概要写作的个性化教育工具中。尽管如此，在将LLMs应用于关键教学场景前，深入理解和有效评估它们的重要性不言而喻，但其空前庞大的尺寸和不断增多的参数却在模型表现欠佳时降低了透明度，阻碍了对其的信任构建。为此，我们携手多名正在研发及应用摘要评分LLMs的学习工程师，通过以人为本的协作设计流程，识别出关于解析模型的核心设计难题和目标，涵盖大篇幅文本输入整合、得分溯源追踪以及提升LLM可解释性技术的规模化应用等环节。为回应这些关切，我们打造了一款名为iScore的交互式可视化分析工具，它使学习工程师能够上传、打分并同时对比多个摘要。紧密结合的视图界面支持用户反复优化摘要的语言表达，即时监控LLM评分变动，并在不同抽象层级上展示模型权重。为了验证这一方案的有效性，我们在一个月的时间内与三位学习工程师共同试用了iScore，并通过一个案例展示，说明借助iScore，一名学习工程师成功提升了其LLM的评分精准度达3个百分点。最后，我们通过定性访谈深入了解这些学习工程师的经验，揭示了iScore如何在实际部署过程中助力他们更好地理解、评估并逐步建立起对LLMs的信任。

> The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.