# 探索大型语言模型供应链：制定研究路线图

发布时间：2024年04月19日

`LLM理论` `人工智能`

> Large Language Model Supply Chain: A Research Agenda

# 摘要

> 随着预训练的大型语言模型（LLMs）和大型多模态模型（LMMs）的突飞猛进，智能应用迎来了新纪元，从自然语言处理到内容创造的各个领域都经历了革新。LLM供应链作为现代人工智能领域的关键一环，它覆盖了预训练模型从起始开发、训练到最终部署和跨领域应用的全过程。本文深入探讨了LLM供应链的三大核心要素：模型基础设施、模型生命周期以及下游应用生态。模型基础设施包括训练、优化和部署所需的数据集和工具链；模型生命周期则覆盖了训练、测试、发布和持续维护；下游应用生态则是指将预训练模型整合到多样化智能应用中的能力。尽管如此，这一迅猛发展的领域在关键环节上仍面临诸多挑战，如数据隐私与安全、模型的可解释性与公正性、基础设施的可扩展性以及法规遵从性。应对这些挑战对于释放LLMs的全部潜力并确保其道德和负责任的使用至关重要。本文提出了LLM供应链的未来研究方向，旨在推动这些变革性LLMs的持续发展和负责任的部署。

> The rapid advancements in pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs) have ushered in a new era of intelligent applications, transforming fields ranging from natural language processing to content generation. The LLM supply chain represents a crucial aspect of the contemporary artificial intelligence landscape. It encompasses the entire lifecycle of pre-trained models, from its initial development and training to its final deployment and application in various domains. This paper presents a comprehensive overview of the LLM supply chain, highlighting its three core elements: 1) the model infrastructure, encompassing datasets and toolchain for training, optimization, and deployment; 2) the model lifecycle, covering training, testing, releasing, and ongoing maintenance; and 3) the downstream application ecosystem, enabling the integration of pre-trained models into a wide range of intelligent applications. However, this rapidly evolving field faces numerous challenges across these key components, including data privacy and security, model interpretability and fairness, infrastructure scalability, and regulatory compliance. Addressing these challenges is essential for harnessing the full potential of LLMs and ensuring their ethical and responsible use. This paper provides a future research agenda for the LLM supply chain, aiming at driving the continued advancement and responsible deployment of these transformative LLMs.

![探索大型语言模型供应链：制定研究路线图](../../../paper_images/2404.12736/x1.png)

[Arxiv](https://arxiv.org/abs/2404.12736)