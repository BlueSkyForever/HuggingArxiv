# LLMChain：一个利用区块链技术构建的声誉系统，旨在促进大型语言模型的共享与评估。

发布时间：2024年04月19日

`LLM应用` `法律援助` `医疗诊断`

> LLMChain: Blockchain-based Reputation System for Sharing and Evaluating Large Language Models

# 摘要

> 大型语言模型（LLMs）在理解、生成和推理语言方面的新挑战和能力迅速提升。尽管它们在自然语言处理应用中表现出色，但也可能表现出幻觉、推理不可靠和生成有害内容等不良行为。这些问题削弱了人们对LLMs的信任，并在法律援助、医学诊断等对精确度、可靠性和道德考虑至关重要的领域中，对LLMs的实际应用构成了阻碍。此外，用户不满情绪的评估和捕捉目前还不够完善。为了有效评估用户对LLMs互动的满意度和信任度，我们设计并开发了LLMChain——一个结合自动评估和人工反馈的去中心化区块链声誉系统，它能为LLMs分配反映其行为的情境声誉分数。LLMChain旨在帮助用户和组织找到最适合他们需求的可信LLM，并为LLM开发者提供改进模型的宝贵信息。据我们所知，这是首次提出使用区块链框架共享和评估LLMs。LLMChain利用最新工具构建，并在两个基准数据集上进行了评估，证明了其在评估七种不同LLMs方面的有效性和可扩展性。

> Large Language Models (LLMs) have witnessed rapid growth in emerging challenges and capabilities of language understanding, generation, and reasoning. Despite their remarkable performance in natural language processing-based applications, LLMs are susceptible to undesirable and erratic behaviors, including hallucinations, unreliable reasoning, and the generation of harmful content. These flawed behaviors undermine trust in LLMs and pose significant hurdles to their adoption in real-world applications, such as legal assistance and medical diagnosis, where precision, reliability, and ethical considerations are paramount. These could also lead to user dissatisfaction, which is currently inadequately assessed and captured. Therefore, to effectively and transparently assess users' satisfaction and trust in their interactions with LLMs, we design and develop LLMChain, a decentralized blockchain-based reputation system that combines automatic evaluation with human feedback to assign contextual reputation scores that accurately reflect LLM's behavior. LLMChain not only helps users and entities identify the most trustworthy LLM for their specific needs, but also provides LLM developers with valuable information to refine and improve their models. To our knowledge, this is the first time that a blockchain-based distributed framework for sharing and evaluating LLMs has been introduced. Implemented using emerging tools, LLMChain is evaluated across two benchmark datasets, showcasing its effectiveness and scalability in assessing seven different LLMs.

[Arxiv](https://arxiv.org/abs/2404.13236)