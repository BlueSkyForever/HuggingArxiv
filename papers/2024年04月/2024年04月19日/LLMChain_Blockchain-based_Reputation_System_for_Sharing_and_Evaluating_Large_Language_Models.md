# LLMChain：一个利用区块链技术构建的声誉系统，旨在促进大型语言模型的共享与评估。

发布时间：2024年04月19日

`LLM应用` `区块链`

> LLMChain: Blockchain-based Reputation System for Sharing and Evaluating Large Language Models

# 摘要

> 大型语言模型（LLMs）在语言理解、生成和推理的新挑战与能力上迅猛发展。尽管它们在自然语言处理应用中表现卓越，却也容易表现出不良行为，如幻觉、推理不可靠以及产生有害内容，这些行为削弱了人们对LLMs的信任，并在法律援助、医学诊断等对精确性、可靠性和伦理考量极为重要的实际应用中构成了障碍。此外，这些行为还可能引起用户不满，而目前对用户不满的评估和捕捉还不够充分。为了有效且透明地评估用户对LLMs互动的满意度和信任度，我们设计并开发了LLMChain——一个基于去中心化区块链的声誉系统。该系统结合自动评估和人工反馈，为用户提供准确的LLM行为声誉评分。LLMChain不仅助力用户和机构找到最适合其需求的可信LLM，还为LLM开发者提供了改进模型的宝贵数据。据我们所知，这是首次提出基于区块链的LLMs共享与评估框架。利用最新工具构建的LLMChain，在两个基准数据集上进行了评估，证明了其在评估七种不同LLMs方面的有效性和可扩展性。

> Large Language Models (LLMs) have witnessed rapid growth in emerging challenges and capabilities of language understanding, generation, and reasoning. Despite their remarkable performance in natural language processing-based applications, LLMs are susceptible to undesirable and erratic behaviors, including hallucinations, unreliable reasoning, and the generation of harmful content. These flawed behaviors undermine trust in LLMs and pose significant hurdles to their adoption in real-world applications, such as legal assistance and medical diagnosis, where precision, reliability, and ethical considerations are paramount. These could also lead to user dissatisfaction, which is currently inadequately assessed and captured. Therefore, to effectively and transparently assess users' satisfaction and trust in their interactions with LLMs, we design and develop LLMChain, a decentralized blockchain-based reputation system that combines automatic evaluation with human feedback to assign contextual reputation scores that accurately reflect LLM's behavior. LLMChain not only helps users and entities identify the most trustworthy LLM for their specific needs, but also provides LLM developers with valuable information to refine and improve their models. To our knowledge, this is the first time that a blockchain-based distributed framework for sharing and evaluating LLMs has been introduced. Implemented using emerging tools, LLMChain is evaluated across two benchmark datasets, showcasing its effectiveness and scalability in assessing seven different LLMs.

[Arxiv](https://arxiv.org/abs/2404.13236)