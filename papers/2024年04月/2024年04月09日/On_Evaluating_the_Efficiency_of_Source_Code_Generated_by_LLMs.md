# 探究大型语言模型所生成源代码的效率评估

发布时间：2024年04月09日

`LLM应用` `软件工程`

> On Evaluating the Efficiency of Source Code Generated by LLMs

# 摘要

> 近年来，大型语言模型（LLMs）在代码生成领域的卓越表现引人注目。不同于以往仅关注代码正确性的研究，我们进一步探讨了代码的效率问题。高效的代码意味着更出色的程序性能和软件运行效率，这对于LLM辅助编程尤为重要。我们首先在HumanEval和MBPP两个基准测试中对LLMs产出的代码效率进行评估。接着，我们挑选了LeetCode在线评判平台上的一系列编程难题，进行更具挑战性的测试。最终，我们研究了一些有效的提示方法，旨在帮助LLMs编写出更加高效的代码。

> Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.

[Arxiv](https://arxiv.org/abs/2404.06041)