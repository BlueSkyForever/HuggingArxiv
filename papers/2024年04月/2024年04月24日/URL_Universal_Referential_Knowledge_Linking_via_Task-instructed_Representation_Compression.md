# URL：通过任务驱动的表征压缩技术，实现知识的普遍引用链接。

发布时间：2024年04月24日

`LLM应用` `信息检索` `知识链接`

> URL: Universal Referential Knowledge Linking via Task-instructed Representation Compression

# 摘要

> 将索赔与确凿的参考信息相连是满足人们对真实性和可靠性信息需求的关键。目前研究多聚焦于信息检索或语义匹配等特定任务，这些任务中的索赔与参考信息关系明确且固定。然而，现实世界中的引用知识链接（RKL）远为多样和复杂。本文提出了一种通用引用知识链接（URL）框架，目的在于通过统一的模型处理多样化的RKL任务。我们引入了一种由大型语言模型（LLM）驱动的任务指导型表示压缩技术和多视角学习方法，以提升LLMs在引用知识链接方面的指令遵循和语义理解能力。此外，我们还建立了一个新的基准测试，用以评估模型在不同情境下的RKL任务表现。实验结果证明，对于现有方法而言，通用RKL颇具挑战性，但我们提出的框架能够有效应对各种情境下的任务，显著优于先前的方法。

> Linking a claim to grounded references is a critical ability to fulfill human demands for authentic and reliable information. Current studies are limited to specific tasks like information retrieval or semantic matching, where the claim-reference relationships are unique and fixed, while the referential knowledge linking (RKL) in real-world can be much more diverse and complex. In this paper, we propose universal referential knowledge linking (URL), which aims to resolve diversified referential knowledge linking tasks by one unified model. To this end, we propose a LLM-driven task-instructed representation compression, as well as a multi-view learning approach, in order to effectively adapt the instruction following and semantic understanding abilities of LLMs to referential knowledge linking. Furthermore, we also construct a new benchmark to evaluate ability of models on referential knowledge linking tasks across different scenarios. Experiments demonstrate that universal RKL is challenging for existing approaches, while the proposed framework can effectively resolve the task across various scenarios, and therefore outperforms previous approaches by a large margin.

![URL：通过任务驱动的表征压缩技术，实现知识的普遍引用链接。](../../../paper_images/2404.16248/x1.png)

![URL：通过任务驱动的表征压缩技术，实现知识的普遍引用链接。](../../../paper_images/2404.16248/x2.png)

[Arxiv](https://arxiv.org/abs/2404.16248)