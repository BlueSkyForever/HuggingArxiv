# 将视觉语言模型打造成智能手机助理。

发布时间：2024年04月12日

`Agent` `移动设备` `人机交互`

> Training a Vision Language Model as Smartphone Assistant

# 摘要

> 为应对数字助手执行多样化用户任务的挑战，我们的研究致力于基于指令的移动设备控制。借助大型语言模型（LLMs）的前沿技术，我们推出了一款视觉语言模型（VLM），它能够在移动设备上应对各种任务。该模型通过与用户界面（UI）的互动来工作，利用屏幕的视觉信息，模拟人类的交互动作，如点击和滑动。这种输入输出的广泛适用性让我们的代理能与设备上任意应用进行互动。区别于传统方法，我们的模型不仅处理单一屏幕图像，还能处理由连续截图及相应动作生成的视觉-语言信息流。在严苛的Android in the Wild基准测试中，我们的方法展现了令人期待的效果和巨大潜力。

> Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.

![将视觉语言模型打造成智能手机助理。](../../../paper_images/2404.08755/x1.png)

[Arxiv](https://arxiv.org/abs/2404.08755)