# 打造一款智能手机助手，其核心是一个视觉语言模型。

发布时间：2024年04月12日

`Agent` `移动设备控制` `人工智能`

> Training a Vision Language Model as Smartphone Assistant

# 摘要

> 为了应对能够处理多种用户任务的数字助手的挑战，我们的研究着眼于基于指令的移动设备控制。借助大型语言模型（LLMs）的最新进展，我们推出了一款视觉语言模型（VLM），能够应对移动设备上的多样化任务。该模型通过与用户界面（UI）的互动来执行任务，利用设备屏幕的视觉输入，模拟人类的交互方式，如点击和滑动等。这种输入输出的通用性使得我们的代理能够与设备上的所有应用程序进行互动。与以往的方法相比，我们的模型不仅处理单个屏幕图像，还能处理由连续的屏幕截图序列及其对应的动作生成的视觉-语言句子。在充满挑战的 Android in the Wild 基准测试中，我们的方法展现了其显著的效率和潜力。

> Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.

![打造一款智能手机助手，其核心是一个视觉语言模型。](../../../paper_images/2404.08755/x1.png)

[Arxiv](https://arxiv.org/abs/2404.08755)