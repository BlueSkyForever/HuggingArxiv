# MedThink：揭示医学视觉问答背后的多模态决策推理过程

发布时间：2024年04月18日

`分类：LLM应用

这篇论文的摘要提到了利用大型语言模型（LLM）来改进医学视觉问答（MedVQA）任务，并通过创建新的基准数据集和优化数据准备来提高模型的透明度和理解性。这表明了LLM在实际应用中的作用，特别是在医学领域，因此应该归类为LLM应用。` `人工智能`

> MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale

# 摘要

> 医学视觉问答（MedVQA）通过图像回答医学问题，为医疗领域带来了突破性进展，助力专家快速解读医学影像，加速诊断流程。但现有MedVQA模型的透明度不足，理解其决策逻辑存在难度。为克服这一难题，我们开发了半自动化注释流程，优化数据准备，并创建了新的基准数据集R-RAD和R-SLAKE。这些数据集结合了多模态大型语言模型和人工注释，为现有MedVQA数据集中的问答对提供医学决策理由，如VQA-RAD和SLAKE。我们还构建了一个新框架，它通过融入医学决策逻辑对轻量级预训练生成模型进行微调，包含三种策略生成决策结果及其理由，使医学决策过程在推理中清晰可见。实验结果表明，该方法在R-RAD和R-SLAKE数据集上的准确率分别达到83.5%和86.3%，超越了现有顶尖模型。相关数据集和代码将公开发布。

> Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamlining data preparation and build new benchmark MedVQA datasets R-RAD and R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales into the training process. The framework includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning. Extensive experiments demonstrate that our method can achieve an accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming existing state-of-the-art baselines. Dataset and code will be released.

[Arxiv](https://arxiv.org/abs/2404.12372)