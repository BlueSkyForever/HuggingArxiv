# MedThink：揭示医学视觉问答背后的多模态决策推理过程

发布时间：2024年04月18日

`分类：LLM应用

这篇论文主要讨论了医学视觉问答（MedVQA）领域中的问题，特别是现有模型的透明度和可解释性不足的问题。为了解决这个问题，作者开发了一种半自动化注释流程，优化了数据准备，并推出了新的基准数据集。此外，作者构建了一个创新框架，通过融入医学决策逻辑，对轻量级预训练生成模型进行微调，以提高模型的透明度和可解释性。这个框架使用了多模态大型语言模型，因此这篇论文可以归类为LLM应用。` `人工智能`

> MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale

# 摘要

> 医学视觉问答（MedVQA）通过图像解答医学问题，为医疗领域带来了技术挑战和进步，助力专家快速解读医学影像，加速诊断流程。但现有MedVQA模型的透明度和可解释性不足，难以洞悉其决策机制。为克服这一难题，我们开发了一种半自动化注释流程，优化数据准备，并推出了新的基准数据集R-RAD与R-SLAKE。这些数据集结合了多模态大型语言模型和人工注释，为现有MedVQA数据集（如VQA-RAD和SLAKE）中的问题-答案对提供了医学决策的中间推理。此外，我们构建了一个创新框架，通过融入医学决策逻辑，对轻量级预训练生成模型进行微调。该框架采用三种策略生成决策结果及其推理，使医学决策过程在推理中清晰可见。实验结果显示，我们的模型在R-RAD数据集上准确度达83.5%，在R-SLAKE上达86.3%，均显著超越了现有顶尖模型。相关数据集和代码将公开发布。

> Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamlining data preparation and build new benchmark MedVQA datasets R-RAD and R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales into the training process. The framework includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning. Extensive experiments demonstrate that our method can achieve an accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming existing state-of-the-art baselines. Dataset and code will be released.

[Arxiv](https://arxiv.org/abs/2404.12372)