# 通过上下文学习，提升大型语言模型在生成常识时的多样性。

发布时间：2024年04月25日

`LLM应用` `人工智能`

> Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning

# 摘要

> 生成性常识推理（GCR）要求模型不仅要运用常识知识进行情境推理，还需产出连贯的语句。生成语句的质量固然关键，但多样性也不可或缺，因为它展现了模型调用不同常识知识的能力。大型语言模型（LLMs）通过上下文学习（ICL）在多种任务中提升了生成质量，且无需额外的微调步骤。尽管如此，LLMs生成结果的多样性尚未经过系统性研究。为此，我们提出了一种新方法，旨在丰富LLMs的生成多样性，同时维持其生成质量。在三个GCR基准数据集上的实验结果显示，该方法在质量和多样性之间取得了完美的平衡。此外，由我们方法生成的语句还可以作为训练素材，以增强现有常识生成器的多样性。

> Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.

[Arxiv](https://arxiv.org/abs/2404.16807)