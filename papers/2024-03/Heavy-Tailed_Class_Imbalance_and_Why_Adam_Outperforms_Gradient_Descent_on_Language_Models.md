#### [《全知计划V2》：旨在攻克开放世界的通用关系理解难题，探索更全面的关系认知能力。](https://arxiv.org/abs/2402.19474)
#### The All-Seeing Project V2: Towards General Relation Comprehension of the Open World
发布时间：2024年02月29日
> 现在呈献给大家的是全新升级的 All-Seeing Project V2，该项目研发出一款专用于理解图像中物体间关系的模型及配套数据集。我们创新性地提出了All-Seeing Model V2 (ASMv2)，巧妙地将文本生成、物体定位和关系理解融入一个名为“关系对话”(ReC)的任务中。得益于这一集成化任务，ASMv2 不仅能敏锐捕捉到图片中所有物体并精准识别，更能深入解析这些物体间的错综复杂关系网络，有效缓解了当前多模态大型语言模型(MMLMs)常面临的“关系臆断”难题。为了促进MMLMs在关系理解领域的训练与评测，我们精心打造了首个高标准的ReC数据集 AS-V2，其格式严格遵循标准指令调优数据的要求。此外，我们还创新设计了一项名为“循环关系探测评估”(CRPE)的新基准测试，以便全面检验MMLMs在关系理解上的实力。尤为突出的是，ASMv2在这一关系认知基准上取得了高达52.04的整体准确率，远超LLaVA-1.5的43.14，展现了显著的优势。我们期待本项目的研究成果能激发更多后续探索，并有力推动通向人工智能普遍智能的步伐。您可访问https://github.com/OpenGVLab/all-seeing获取我们的项目资源。
> We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data. In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at https://github.com/OpenGVLab/all-seeing.
Agent
#### [为探究大型语言模型（LLM）可信度动态变化，我们再次聚焦其预训练阶段，旨在深入理解这一关键阶段对其后续表现和可信度的影响。](https://arxiv.org/abs/2402.19465)
#### Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
发布时间：2024年02月29日
> 对于LLMs来说，确保其可信性是至关重要的。当前多数研究聚焦于完全预训练阶段的LLMs，以求深化理解并改进其可信性。在这篇论文中，我们独辟蹊径，首度探讨了LLMs在预训练阶段潜在的可信性表现，尤其关注五个核心维度：可靠性、隐私保护、无害性、公正性和稳健性。实验初期，我们采用线性探测方法对LLMs进行分析，发现高探测精度意味着LLMs在预训练早期就能识别出各可信维度中的概念差异。因此，为进一步挖掘预训练过程中的潜在可能性，我们从LLM的预训练检查点提取导向向量来提升其可信性。此外，借鉴\citet{choi2023understanding}关于互信息估计受限于线性探测精度的观点，我们运用互信息探测手段，研究LLMs在预训练期间可信性动态变化规律。我们首次揭示了一种类似“拟合与压缩”双阶段的现象~\citep{shwartz2017opening}。此研究开创性地剖析了LLM预训练阶段的可信性建模问题，旨在揭示更多新见解，并激励该领域的后续进展。我们的代码将在GitHub上公开，网址为 \url{https://github.com/ChnQ/TracingLLM}。
> Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \url{https://github.com/ChnQ/TracingLLM}.
LLM理论
#### [为探究大型语言模型的安全性与潜在漏洞，我们引入了“好奇心驱动的红队”策略，通过模拟攻击者对大型语言模型进行挑战和检验，以挖掘其可能存在的问题。](https://arxiv.org/abs/2402.19464)
#### Curiosity-driven Red-teaming for Large Language Models
发布时间：2024年02月29日
> 尽管LLMs在各类自然语言应用中蕴含无限可能，却有误生不当或有毒内容的风险。目前，检测LLM何时生成不受欢迎内容的方法是组织一支人类测试团队，即“红队”，来设计能触发LLM不良回应的输入提示。然而，单靠人力测试成本高昂且耗时。近期研究借助强化学习（RL），训练了一个专门的红队LLM，自动寻找最可能诱发目标LLM不良反应的测试案例。不过，现存的RL技术只能产出少量有效的测试案例，难以全面覆盖可能导致LLM输出不良信息的所有提示情境。为此，我们借鉴了广受研究的好奇心驱动探索原理——追求新颖性的方法，以此提升生成测试案例的覆盖率。我们提出的好奇心驱动红队策略（CRT）在扩大测试案例覆盖面的同时，保证甚至提升了它们的有效性。实验表明，CRT成功激发出已深度优化并依据人类偏好调整以避免毒性输出的LLaMA2模型产生毒性回应。相关代码已发布于 \url{https://github.com/Improbable-AI/curiosity_redteam}。
> Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \url{https://github.com/Improbable-AI/curiosity_redteam}
LLM应用
#### [在语言模型训练中，Adam 优化器相较于梯度下降法展现出更优性能，这一现象与重尾类别不平衡问题密切相关。本研究探讨了这一特性背后的原因，并揭示了 Adam 如何更好地应对重尾类别不平衡对语言模型训练的影响。](https://arxiv.org/abs/2402.19449)
#### Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
发布时间：2024年02月29日
> 研究表明，Adam 在优化大型语言转换器时明显胜过梯度下降，尤其是在处理不均衡的词汇频率分布时优势更加突出，然而其中原因尚未明晰。实际上，语言建模任务中存在的严重类别不平衡问题对优化过程产生了挑战，梯度下降时，低频词的损失下降速度往往滞后于高频词。鉴于大部分样本来源于低频词，所以采用梯度下降时，整体平均损失的下降速率较慢。而 Adam 以及基于符号的方法则成功规避了这一难题，对所有类别都实现了预测性能的提升。为了确证类别不平衡是造成这一现象的关键因素，我们不仅在语言转换器领域，还涵盖了视觉CNNs和线性模型等多种模型结构与数据类型，验证了这一现象的普遍存在。此外，通过在交叉熵损失的线性分类问题上深入探究，我们揭示了重尾类别不平衡会引发病态条件，而 Adam 所采用的归一化手段能够有效抵消这一不利影响。
> Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it.
LLM理论