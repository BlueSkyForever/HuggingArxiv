#### [《全知计划V2》：旨在攻克开放世界的通用关系理解难题，探索更全面的关系认知能力。](https://arxiv.org/abs/2402.19474)
#### The All-Seeing Project V2: Towards General Relation Comprehension of the Open World
发布时间：2024年02月29日
> 现在呈献给大家的是全新升级的 All-Seeing Project V2，该项目研发出一款专用于理解图像中物体间关系的模型及配套数据集。我们创新性地提出了All-Seeing Model V2 (ASMv2)，巧妙地将文本生成、物体定位和关系理解融入一个名为“关系对话”(ReC)的任务中。得益于这一集成化任务，ASMv2 不仅能敏锐捕捉到图片中所有物体并精准识别，更能深入解析这些物体间的错综复杂关系网络，有效缓解了当前多模态大型语言模型(MMLMs)常面临的“关系臆断”难题。为了促进MMLMs在关系理解领域的训练与评测，我们精心打造了首个高标准的ReC数据集 AS-V2，其格式严格遵循标准指令调优数据的要求。此外，我们还创新设计了一项名为“循环关系探测评估”(CRPE)的新基准测试，以便全面检验MMLMs在关系理解上的实力。尤为突出的是，ASMv2在这一关系认知基准上取得了高达52.04的整体准确率，远超LLaVA-1.5的43.14，展现了显著的优势。我们期待本项目的研究成果能激发更多后续探索，并有力推动通向人工智能普遍智能的步伐。您可访问https://github.com/OpenGVLab/all-seeing获取我们的项目资源。
> We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data. In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at https://github.com/OpenGVLab/all-seeing.
Agent
#### [为探究大型语言模型（LLM）可信度动态变化，我们再次聚焦其预训练阶段，旨在深入理解这一关键阶段对其后续表现和可信度的影响。](https://arxiv.org/abs/2402.19465)
#### Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
发布时间：2024年02月29日
> 对于LLMs来说，确保其可信性是至关重要的。当前多数研究聚焦于完全预训练阶段的LLMs，以求深化理解并改进其可信性。在这篇论文中，我们独辟蹊径，首度探讨了LLMs在预训练阶段潜在的可信性表现，尤其关注五个核心维度：可靠性、隐私保护、无害性、公正性和稳健性。实验初期，我们采用线性探测方法对LLMs进行分析，发现高探测精度意味着LLMs在预训练早期就能识别出各可信维度中的概念差异。因此，为进一步挖掘预训练过程中的潜在可能性，我们从LLM的预训练检查点提取导向向量来提升其可信性。此外，借鉴\citet{choi2023understanding}关于互信息估计受限于线性探测精度的观点，我们运用互信息探测手段，研究LLMs在预训练期间可信性动态变化规律。我们首次揭示了一种类似“拟合与压缩”双阶段的现象~\citep{shwartz2017opening}。此研究开创性地剖析了LLM预训练阶段的可信性建模问题，旨在揭示更多新见解，并激励该领域的后续进展。我们的代码将在GitHub上公开，网址为 \url{https://github.com/ChnQ/TracingLLM}。
> Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \url{https://github.com/ChnQ/TracingLLM}.
LLM理论
#### [为探究大型语言模型的安全性与潜在漏洞，我们引入了“好奇心驱动的红队”策略，通过模拟攻击者对大型语言模型进行挑战和检验，以挖掘其可能存在的问题。](https://arxiv.org/abs/2402.19464)
#### Curiosity-driven Red-teaming for Large Language Models
发布时间：2024年02月29日
> 尽管LLMs在各类自然语言应用中蕴含无限可能，却有误生不当或有毒内容的风险。目前，检测LLM何时生成不受欢迎内容的方法是组织一支人类测试团队，即“红队”，来设计能触发LLM不良回应的输入提示。然而，单靠人力测试成本高昂且耗时。近期研究借助强化学习（RL），训练了一个专门的红队LLM，自动寻找最可能诱发目标LLM不良反应的测试案例。不过，现存的RL技术只能产出少量有效的测试案例，难以全面覆盖可能导致LLM输出不良信息的所有提示情境。为此，我们借鉴了广受研究的好奇心驱动探索原理——追求新颖性的方法，以此提升生成测试案例的覆盖率。我们提出的好奇心驱动红队策略（CRT）在扩大测试案例覆盖面的同时，保证甚至提升了它们的有效性。实验表明，CRT成功激发出已深度优化并依据人类偏好调整以避免毒性输出的LLaMA2模型产生毒性回应。相关代码已发布于 \url{https://github.com/Improbable-AI/curiosity_redteam}。
> Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \url{https://github.com/Improbable-AI/curiosity_redteam}
LLM应用
#### [在语言模型训练中，Adam 优化器相较于梯度下降法展现出更优性能，这一现象与重尾类别不平衡问题密切相关。本研究探讨了这一特性背后的原因，并揭示了 Adam 如何更好地应对重尾类别不平衡对语言模型训练的影响。](https://arxiv.org/abs/2402.19449)
#### Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
发布时间：2024年02月29日
> 研究表明，Adam 在优化大型语言转换器时明显胜过梯度下降，尤其是在处理不均衡的词汇频率分布时优势更加突出，然而其中原因尚未明晰。实际上，语言建模任务中存在的严重类别不平衡问题对优化过程产生了挑战，梯度下降时，低频词的损失下降速度往往滞后于高频词。鉴于大部分样本来源于低频词，所以采用梯度下降时，整体平均损失的下降速率较慢。而 Adam 以及基于符号的方法则成功规避了这一难题，对所有类别都实现了预测性能的提升。为了确证类别不平衡是造成这一现象的关键因素，我们不仅在语言转换器领域，还涵盖了视觉CNNs和线性模型等多种模型结构与数据类型，验证了这一现象的普遍存在。此外，通过在交叉熵损失的线性分类问题上深入探究，我们揭示了重尾类别不平衡会引发病态条件，而 Adam 所采用的归一化手段能够有效抵消这一不利影响。
> Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it.
LLM理论
#### [ArCHer 是一种创新方法，通过运用分层多轮强化学习技术来训练语言模型智能体，使其在复杂对话场景中更具交互性和适应性。](https://arxiv.org/abs/2402.19446)
#### ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL
发布时间：2024年02月29日
> LLMs广泛应用于涉及多轮交互和智慧决策的“智能体”任务中，而强化学习（RL）为解决此类任务提供了通用方案。然而，目前针对LLMs的RL技术大多侧重于优化单次交互的奖励。实际上，大部分单次交互RL方法尚不能让LLMs具备在多轮交互中聪明地搜寻信息、合理分配功劳或反思过往行动的能力，而这对于完成智能体任务至关重要。那么，怎样才能设计出针对LLMs高效且有效的多轮RL算法呢？本文提出了一个专门面向LLMs微调的多轮RL算法构建框架，它不仅保持了现有单轮RL方法（如近端策略优化法）的灵活性，还能有效应对多轮交互、长远考虑及延迟奖励等问题。该框架采用分层RL策略，同时运行两个RL算法：一个是负责跨对话轮次累积奖励的高层离线价值型RL算法；另一个是基于高层价值函数，在每一轮对话或表述内训练令牌策略的低层RL算法。这一被称为ArCHer的分层Actor-Critic框架，还可启发其他的RL方法。实验表明，ArCHer在智能体任务上的表现显著优于现有方法，样本效率提高了大约100倍，并且随着模型容量增大至我们所测试的70亿级别，其性能也持续提升。
> A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).
Agent
#### [为实现高效库导向的代码生成，我们提出了一种组合式API推荐方法。该方法旨在探索和推荐适用于此类任务的恰当API组合，以提升编程效率及代码质量。](https://arxiv.org/abs/2402.19431)
#### Compositional API Recommendation for Library-Oriented Code Generation
发布时间：2024年02月29日
> 尽管LLMs在代码生成领域表现出色，但面对未在训练数据中出现过的库时，生成面向库的代码能力仍有欠缺。过去的研究尝试借助API推荐技术引导LLMs使用库，即找出符合用户需求的API作为输入提示。然而，由于实际开发需求往往涉及多条细粒度API的综合应用，粒度匹配难题使API推荐成为一项棘手工作。因此，我们创新性地提出了CAPIR方案，采用“化整为零”的策略解决粗略需求下的API推荐问题。CAPIR首先利用基于LLM的分解器将大块任务细化为一系列小任务，再通过嵌入式检索器找到与各小任务相匹配的API。同时，CAPIR依靠LLM优化后的重排序机制筛选掉冗余API，确保最终推荐的精准性。为了更准确评估针对粗粒度需求的API推荐效果，我们构建了两个难度较高的基准测试——RAPID（基于文档推荐API）和LOCG（面向库的代码生成）。实验证明，在这两个基准上，CAPIR相比于现有基准方法表现出了显著优势。以RAPID中的Torchdata-AR数据集为例，CAPIR相对于当前最先进API推荐方法，成功将召回率@5提升了约24.5个百分点至43.2%，并将精确率@5提高了约21.6个百分点达到37.1%；而在LOCG的Torchdata-Code数据集中，相较于没有采用API推荐的代码生成方式，CAPIR将pass@100指标提高了大约12个百分点，达到了28.0%。
> Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.
LLM应用
#### [精心打造知识：深入探索聊天式搜索引擎背后的创新运作机制](https://arxiv.org/abs/2402.19421)
#### Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines
发布时间：2024年02月29日
> 搜索引擎作为数字信息传播的核心中介，将信息需求者与供给者紧密相连。以Bing Chat为代表的搭载大型语言模型（LLMs）及检索增强生成（RAG）技术的聊天型搜索引擎，则为搜索生态带来了革新性的跃进。这类引擎展现出类似人类的理解力与创造力，能解读网页信息并精巧回应。然而，LLMs内在的复杂性使得其“思考”过程如同黑箱，即使是设计者也难以完全洞察。本研究致力于深入探究LLM驱动的聊天式搜索引擎（以Bing Chat为例）如何选取信息源生成回答。为了实现这一目标，我们通过与新Bing的实际互动，积累了一个大规模数据集，其中包含它引用的网站及其与常规搜索引擎所列网站的对比情况。运用自然语言处理技术分析发现，Bing Chat更偏好那些既易读、结构规整且困惑度较低的网页内容，这意味着其有独特倾向去选用底层LLM容易预测的文本。我们进一步通过与基于GPT-4知识检索API的互动，收集更多数据，显示了RAG API与Bing Chat在文本偏好的一致性，暗示这类偏好其实源自于底层语言模型的固有特性，而非Bing Chat开发团队刻意编排的结果。此外，我们的研究还揭示了相比传统搜索引擎高排名的网站，RAG技术引用的网站间具有更高的相似度。
> In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their "cognitive" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines.
RAG